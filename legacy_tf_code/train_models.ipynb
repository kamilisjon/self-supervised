{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa80770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, roc_auc_score\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "DATA_ROOT = \"/home/gpu1/Desktop/kamilio/Pneumothorax\"\n",
    "MODEL_ROOT = \"/home/gpu1/Desktop/kamilio/models\"\n",
    "IMG_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9eddfd",
   "metadata": {},
   "source": [
    "## Load necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27622027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "from functools import partial\n",
    "\n",
    "transforms = A.Compose([\n",
    "    A.HorizontalFlip(0.5),\n",
    "    A.VerticalFlip(0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.3, scale_limit=0.50, rotate_limit=(-90,90), p=.75)\n",
    "])\n",
    "\n",
    "def get_classes(dataset_name):\n",
    "    #TODO add functionality to pop error if classes in data splits do not match\n",
    "    class_names = os.listdir(os.path.join(DATA_ROOT, dataset_name, \"train\"))\n",
    "    class_names = [int(name) for name in class_names]\n",
    "    class_names.sort()\n",
    "    class_names = [str(name) for name in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_classes_dict(dataset_name):\n",
    "    class_names = get_classes(dataset_name)\n",
    "    class_names_dict = {}\n",
    "    for class_name in class_names:\n",
    "        image_sample_name = os.path.basename(glob.glob(os.path.join(DATA_ROOT, dataset_name, \"train\", str(class_name),'*'))[0])\n",
    "        class_names_dict[str(class_name)] = image_sample_name.split('_')[1]\n",
    "    return class_names_dict\n",
    "\n",
    "def get_classes_dict_empty_lists(dataset_name):\n",
    "    class_names = get_classes(dataset_name)\n",
    "    class_names_dict = {}\n",
    "    for class_name in class_names:\n",
    "        class_names_dict[str(class_name)] = []\n",
    "    return class_names_dict\n",
    "\n",
    "def get_label(file_path):\n",
    "    # Convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    # TODO: pass CLASS_NAMES as function parameter\n",
    "    one_hot = parts[-2] == CLASS_NAMES\n",
    "    # Integer encode the label\n",
    "    return int(one_hot)\n",
    "\n",
    "def decode_img(img):\n",
    "    # Convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    return img\n",
    "\n",
    "def aug_fn(image):\n",
    "    data = {\"image\":image}\n",
    "    aug_data = transforms(**data)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    return aug_img\n",
    "\n",
    "def apply_transformations(image):\n",
    "    aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n",
    "    return aug_img\n",
    "\n",
    "def process_path(file_path, use_augmentations: bool):\n",
    "    label = get_label(file_path)\n",
    "    # Load the raw data from the file as a string\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = decode_img(image)\n",
    "    # Resize and norm the image\n",
    "    image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n",
    "    image = tf.cast(image/255, tf.float32)\n",
    "    if use_augmentations:\n",
    "        image = apply_transformations(image)\n",
    "    return image, label\n",
    "\n",
    "def load_dataset(dataset_name:str, split_type: str, use_transformations: bool, mini_batch_size: int, shuffle: bool):\n",
    "    global CLASS_NAMES\n",
    "    CLASS_NAMES = get_classes(dataset_name)\n",
    "    dataset = tf.data.Dataset.list_files(f\"{DATA_ROOT}/{dataset_name}/{split_type}/*/*\", shuffle=False)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(dataset), reshuffle_each_iteration=False)\n",
    "    dataset = dataset.map(partial(process_path, use_augmentations=use_transformations), num_parallel_calls=AUTOTUNE).batch(mini_batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b097b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.applications import EfficientNetB5\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_initial_weights_type(imagenet_pretrained_backbone: bool):\n",
    "    if imagenet_pretrained_backbone:\n",
    "        weights='imagenet'\n",
    "    else:\n",
    "        weights=None\n",
    "    return weights\n",
    "\n",
    "def rebuild_top(model, class_count: int, top_dropout_rate: float = 0):\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.BatchNormalization(name='batch_normalization')(x)\n",
    "    if top_dropout_rate != 0:\n",
    "        x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(class_count, activation=\"softmax\", name=\"pred\")(x)\n",
    "    return outputs\n",
    "\n",
    "def build_EffNetB0(class_count: int, imagenet_pretrained_backbone: bool, top_dropout_rate: float = 0):\n",
    "    \n",
    "    weights_type = get_initial_weights_type(imagenet_pretrained_backbone)\n",
    "    \n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=weights_type)\n",
    "    outputs = rebuild_top(model, class_count, top_dropout_rate)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='EfficientNetB0')\n",
    "    print(\"EffNet0 model build successfull\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_EffNetB3(class_count: int, imagenet_pretrained_backbone: bool, top_dropout_rate: float = 0):\n",
    "    \n",
    "    weights_type = get_initial_weights_type(imagenet_pretrained_backbone)\n",
    "    \n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    model = EfficientNetB3(include_top=False, input_tensor=inputs, weights=weights_type)\n",
    "    outputs = rebuild_top(model, class_count, top_dropout_rate)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='EfficientNetB3')\n",
    "    print(\"EffNet3 model build successfull\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_EffNetB5(class_count: int, imagenet_pretrained_backbone: bool, top_dropout_rate: float = 0):\n",
    "    \n",
    "    weights_type = get_initial_weights_type(imagenet_pretrained_backbone)\n",
    "    \n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    model = EfficientNetB5(include_top=False, input_tensor=inputs, weights=weights_type)\n",
    "    outputs = rebuild_top(model, class_count, top_dropout_rate)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='EfficientNetB5')\n",
    "    print(\"EffNet5 model build successfull\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_model(class_count: int, imagenet_pretrained_backbone: bool, architecture: str,  top_dropout_rate: float = 0):\n",
    "    if architecture == \"EffNetB0\":\n",
    "        model = build_EffNetB0(class_count, imagenet_pretrained_backbone, top_dropout_rate)\n",
    "    elif architecture == \"EffNetB3\":\n",
    "        model = build_EffNetB3(class_count, imagenet_pretrained_backbone, top_dropout_rate)\n",
    "    elif architecture == \"EffNetB5\":\n",
    "        model = build_EffNetB5(class_count, imagenet_pretrained_backbone, top_dropout_rate)\n",
    "    else:\n",
    "        raise Exception(f\"Specified model architecture is not available\")\n",
    "        \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a16005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, test_ds, predictions_path):\n",
    "        super().__init__()\n",
    "        y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "        self.df = pd.DataFrame({\"true\": y_true[:,1]})\n",
    "        if os.path.isfile(predictions_path):\n",
    "            raise Exception(f\"Predictions file was about to be deleted.;(( Be more carefull\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(test_ds)\n",
    "        self.df[str(epoch)] = y_pred[:,1]\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.df.to_csv(predictions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d34196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_loss(hist, title: str, output_file: str):\n",
    "    best_epoch = np.argmin(hist.history[\"val_loss\"])\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(hist.history[\"loss\"])\n",
    "    plt.plot(hist.history[\"val_loss\"])\n",
    "    plt.axvline(x=best_epoch, color='r', linestyle='--', label=\"Best epoch\")\n",
    "    plt.annotate(f'Best epoch: {best_epoch}', xy=(best_epoch, hist.history[\"val_loss\"][best_epoch]),\n",
    "                 xytext=(best_epoch, hist.history[\"val_loss\"][best_epoch] + 0.05),\n",
    "                 arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend([\"Train\", \"Validation\", \"Best epoch\"], loc=\"upper left\")\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712defe8",
   "metadata": {},
   "source": [
    "## Visiaulize samples from downstream taskdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"data_v4.0.0\"\n",
    "\n",
    "map_dict = {'0': 'negative', '1': 'positive'}\n",
    "sample_dict = {'0': [], '1': []}\n",
    "\n",
    "rot_train_dataset_eg = load_dataset(\n",
    "                              dataset_name = dataset_name, \n",
    "                              split_type = \"train\", \n",
    "                              use_transformations = True, \n",
    "                              mini_batch_size = 1,\n",
    "                              shuffle = True)\n",
    "\n",
    "iterator=iter(rot_train_dataset_eg)\n",
    "\n",
    "for _ in range(50):\n",
    "    sample = next(iterator)\n",
    "    image = sample[0].numpy().reshape(IMG_SIZE,IMG_SIZE,3)\n",
    "    label = tf.math.argmax(sample[1], axis=1).numpy()[0]\n",
    "    sample_dict[str(label)].append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = len(sample_dict)\n",
    "plt.figure(figsize=(class_count*5,4))\n",
    "i=1\n",
    "for row in range(class_count):\n",
    "    for column in range(class_count):\n",
    "        plt.subplot(class_count, class_count*5, i)\n",
    "        image = sample_dict[str(row)][column]\n",
    "        classs = map_dict[str(row)]\n",
    "        plt.imshow(image)\n",
    "        plt.title(f'{classs}')\n",
    "        plt.axis('off')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7de835",
   "metadata": {},
   "source": [
    "## Fine-tune pretrained model for brain tumor classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc2c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "configA={\n",
    "     \"type\": \"Downstream\",\n",
    "     \"task\": \"\",\n",
    "     \"dataset_origin\": \"Pneumothorax\",\n",
    "     \"dataset_name\": \"data_v4.0.0\",\n",
    "     \"SSL_checkpoint\": \"041\",\n",
    "     \"use_transformations\": True,\n",
    "     \"frozen_backbone\": False,\n",
    "     \"loss_function\": \"categorical_crossentropy\",\n",
    "     \"top_dropout_rate\": 0.2,\n",
    "     \"learning_rate\": 0.0001,\n",
    "     \"batch_size\": 32,\n",
    "     \"epochs\": 1000,\n",
    "     \"early_stop_patience\": 200}\n",
    "\n",
    "configB={\n",
    "     \"type\": \"Downstream\",\n",
    "     \"task\": \"\",\n",
    "     \"dataset_origin\": \"Pneumothorax\",\n",
    "     \"dataset_name\": \"data_v4.0.1\",\n",
    "     \"SSL_checkpoint\": \"041\",\n",
    "     \"use_transformations\": True,\n",
    "     \"frozen_backbone\": False,\n",
    "     \"loss_function\": \"categorical_crossentropy\",\n",
    "     \"top_dropout_rate\": 0.2,\n",
    "     \"learning_rate\": 0.0001,\n",
    "     \"batch_size\": 32,\n",
    "     \"epochs\": 1000,\n",
    "     \"early_stop_patience\": 200}\n",
    "\n",
    "configC={\n",
    "     \"type\": \"Downstream\",\n",
    "     \"task\": \"\",\n",
    "     \"dataset_origin\": \"Pneumothorax\",\n",
    "     \"dataset_name\": \"data_v4.0.2\",\n",
    "     \"SSL_checkpoint\": \"041\",\n",
    "     \"use_transformations\": True,\n",
    "     \"frozen_backbone\": False,\n",
    "     \"loss_function\": \"categorical_crossentropy\",\n",
    "     \"top_dropout_rate\": 0.2,\n",
    "     \"learning_rate\": 0.0001,\n",
    "     \"batch_size\": 32,\n",
    "     \"epochs\":1000,\n",
    "     \"early_stop_patience\": 200}\n",
    "\n",
    "configD={\n",
    "     \"type\": \"Downstream\",\n",
    "     \"task\": \"\",\n",
    "     \"dataset_origin\": \"Pneumothorax\",\n",
    "     \"dataset_name\": \"data_v4.0.3\",\n",
    "     \"SSL_checkpoint\": \"041\",\n",
    "     \"use_transformations\": True,\n",
    "     \"frozen_backbone\": False,\n",
    "     \"loss_function\": \"categorical_crossentropy\",\n",
    "     \"top_dropout_rate\": 0.2,\n",
    "     \"learning_rate\": 0.0001,\n",
    "     \"batch_size\": 32,\n",
    "     \"epochs\":1000,\n",
    "     \"early_stop_patience\": 200}\n",
    "\n",
    "\n",
    "configs = {\"1\": configA, \"2\": configB, \"3\": configC, \"4\": configD}\n",
    "\n",
    "architectures = {\"3\": \"EffNetB3\"}\n",
    "initial_weigths = {\"ImageNet\": [], \"Pre-text\": [3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa3013e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for major in [10]:    \n",
    "    for minor in [3]:  \n",
    "        for micro in [1, 2, 3, 4]:\n",
    "            for patch in [0]:\n",
    "                model_name = f\"model_v{str(major)}.{str(minor)}.{str(micro)}.{str(patch)}\"\n",
    "                \n",
    "                run = wandb.init(entity = 'ssl_bakalauras',\n",
    "                                 project=f\"Downstream_v{str(major)}\",\n",
    "                                 name = model_name,\n",
    "                                 config = configs[str(micro)])\n",
    "                \n",
    "\n",
    "                print(configs[str(micro)])\n",
    "                print(f\"\\nModel version: {model_name}\\n\")\n",
    "                print(f\"{architectures[str(minor)]} architecture will be used\")\n",
    "                config = wandb.config\n",
    "                project_name = run.project\n",
    "                \n",
    "                path_to_model = os.path.join(MODEL_ROOT, project_name, model_name)\n",
    "\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                class_names = get_classes(config.dataset_name)\n",
    "                print(f\"\\nClasses: {class_names}\\n\")\n",
    "                if config.use_transformations:\n",
    "                    transformations = str(transforms)\n",
    "                    print(f\"Using transfromations for train dataset:\\n{transformations}\\n\")\n",
    "                else:\n",
    "                    transformations = None\n",
    "                    print(f\"No transformations will be used\")\n",
    "\n",
    "\n",
    "        # Load datasets\n",
    "\n",
    "                train_ds = load_dataset(\n",
    "                                      dataset_name = config.dataset_name, \n",
    "                                      split_type = \"train\", \n",
    "                                      use_transformations = config.use_transformations, \n",
    "                                      mini_batch_size = config.batch_size,\n",
    "                                      shuffle = True)\n",
    "\n",
    "                val_ds = load_dataset(\n",
    "                                      dataset_name = config.dataset_name, \n",
    "                                      split_type = \"val\", \n",
    "                                      use_transformations = False, \n",
    "                                      mini_batch_size = config.batch_size,\n",
    "                                      shuffle = True)\n",
    "\n",
    "                test_ds = load_dataset(\n",
    "                                      dataset_name = config.dataset_name, \n",
    "                                      split_type = \"test\", \n",
    "                                      use_transformations = False, \n",
    "                                      mini_batch_size = config.batch_size,\n",
    "                                      shuffle = False)\n",
    "        # Initialize model\n",
    "\n",
    "\n",
    "                if minor in initial_weigths[\"ImageNet\"] and micro == 0:\n",
    "                    upstream_model_type = \"No_model__from_scratch\"\n",
    "                    pstream_model_version = \"None\"\n",
    "                    model = build_model(\n",
    "                                    class_count = len(class_names), \n",
    "                                    imagenet_pretrained_backbone = False,\n",
    "                                    architecture = architectures[str(minor)],\n",
    "                                    top_dropout_rate = config.top_dropout_rate)\n",
    "                    print(f\"Successfully initialized {architectures[str(minor)]} model from scratch\")\n",
    "                elif minor in initial_weigths[\"ImageNet\"] and micro !=0:\n",
    "                    upstream_model_type = f\"ImageNet\"\n",
    "                    upstream_model_version = \"None\"\n",
    "                    model = build_model(\n",
    "                                    class_count = len(class_names), \n",
    "                                    imagenet_pretrained_backbone = True,\n",
    "                                    architecture = architectures[str(minor)],\n",
    "                                    top_dropout_rate = config.top_dropout_rate)\n",
    "                    print(f\"Succesfully loaded weights of ImageNet pretrained {architectures[str(minor)]} model as initial weights\")\n",
    "                elif minor in initial_weigths[\"Pre-text\"]:\n",
    "                    upstream_model_type = \"SSL\"\n",
    "                    upstream_model_version = f\"model_v{str(major)}.{str(minor)}.0\"\n",
    "                    path_to_upstream_model = os.path.join(MODEL_ROOT, f\"SSL_v{str(major)}\",upstream_model_version)\n",
    "                    model = tf.keras.models.load_model(path_to_upstream_model)\n",
    "                    print(f\"Succesfully loaded weights of SSL {upstream_model_version} model\")\n",
    "                    if config.SSL_checkpoint:\n",
    "                        model.load_weights(os.path.join(path_to_upstream_model, 'checkpoints', f\"weights00000{str(config.SSL_checkpoint)}.ckpt\"))\n",
    "                        print(f\"Succesfully loaded SSL checkpoint of epoch: {str(config.SSL_checkpoint)}\")\n",
    "                    # Reinitializing dense classification layer\n",
    "                    inputs = model.input\n",
    "                    x = model.layers[-5].output\n",
    "                    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "                    x = layers.BatchNormalization(name='batch_normalization')(x)\n",
    "                    top_dropout_rate = config.top_dropout_rate\n",
    "                    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "                    outputs = layers.Dense(len(class_names), activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "                    model = tf.keras.Model(inputs, outputs, name=architectures[str(minor)])\n",
    "                    print(f\"Succesfully changed model head\")\n",
    "                else:\n",
    "                    raise Exception(\"Failed to load upstream model\")\n",
    "\n",
    "                wandb.log({\n",
    "                    \"\"\n",
    "                    \"architecture\": architectures[str(minor)],\n",
    "                    \"class_names\": class_names,\n",
    "                    \"transformations\": transformations,\n",
    "                    \"upstream_model_type\": upstream_model_type,\n",
    "                    \"upstream_model_version\":upstream_model_version\n",
    "                })\n",
    "\n",
    "                # Freezing the Convolutional Layers while keeping Dense layers as Trainable\n",
    "                if config.frozen_backbone:\n",
    "                    for layer in model.layers:\n",
    "                        if not (layer.name in ['pred', 'batch_normalization']):\n",
    "                            layer.trainable=False\n",
    "                        else:\n",
    "                            layer.trainable=True\n",
    "                    print(\"Model backbone layers were frozen successfully\\n\")\n",
    "                else:\n",
    "                    print(\"None of the model layers are frozen\\n\")\n",
    "\n",
    "        # Compile model\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
    "                model.compile(optimizer=optimizer, loss=config.loss_function, metrics = ['acc', tf.keras.metrics.AUC()])\n",
    "\n",
    "        # Train the model\n",
    "                predictions_path = os.path.join(DATA_ROOT, \"test_ds_predictions\", f'{model_name}-{str(config.early_stop_patience)}.csv')\n",
    "                model.summary()\n",
    "                history = model.fit(train_ds,\n",
    "                          epochs=config.epochs,\n",
    "                          validation_data=val_ds,\n",
    "                          callbacks=[WandbCallback(save_model=(False)),\n",
    "                                     TestCallback(test_ds, predictions_path),\n",
    "                                     tf.keras.callbacks.EarlyStopping(patience=config.early_stop_patience, \n",
    "                                                                      restore_best_weights=True)])\n",
    "                \n",
    "                plot_loss(history, \"Downstream model loss graphs\",  f\"{path_to_model}_seaborn.png\")\n",
    "\n",
    "        # Test the model \n",
    "\n",
    "                test_loss, test_acc, _ = model.evaluate(test_ds)\n",
    "                y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "                y_prob = model.predict(test_ds)\n",
    "                test_auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "\n",
    "                test_ds = load_dataset(\n",
    "                          dataset_name = config.dataset_name, \n",
    "                          split_type = \"test\", \n",
    "                          use_transformations = False, \n",
    "                          mini_batch_size = 1,\n",
    "                          shuffle=True)\n",
    "\n",
    "                y_true = []\n",
    "                y_pred = []\n",
    "                for sample in test_ds:\n",
    "                    image = sample[0]\n",
    "                    y_true.append(tf.math.argmax(sample[1], axis=1).numpy()[0])\n",
    "                    y_pred.append(tf.math.argmax(model.predict(image), axis=1).numpy()[0])\n",
    "                print(classification_report(y_true, y_pred))\n",
    "\n",
    "                wandb.log({\n",
    "                        \"test_loss\": test_loss,\n",
    "                        \"test_acc\": test_acc,\n",
    "                        \"test_auc\": test_auc,\n",
    "                        \"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                            y_true=y_true, preds=y_pred,\n",
    "                            class_names=class_names)\n",
    "                })\n",
    "\n",
    "        # Save the model    \n",
    "\n",
    "                if minor in initial_weigths[\"ImageNet\"]:\n",
    "                    model.save_weights(os.path.join(path_to_model, \"checkpoint\"))\n",
    "                elif minor in initial_weigths[\"Pre-text\"]:\n",
    "                    model.save(path_to_model)\n",
    "\n",
    "                run.finish()\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
