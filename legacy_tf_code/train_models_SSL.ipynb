{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fa80770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, roc_auc_score\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "DATA_ROOT = \"/home/gpu1/Desktop/kamilio/Pneumothorax\"\n",
    "MODEL_ROOT = \"/home/gpu1/Desktop/kamilio/models\"\n",
    "IMG_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9eddfd",
   "metadata": {},
   "source": [
    "## Load necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27622027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "from functools import partial\n",
    "\n",
    "transforms = A.Compose([\n",
    "    A.Affine(scale = (1.4, 1.8), keep_ratio=True, p=0.95),\n",
    "    A.GridDistortion(distort_limit=0.5, p=0.75)\n",
    "])\n",
    "\n",
    "def get_classes(dataset_name):\n",
    "    #TODO add functionality to pop error if classes in data splits do not match\n",
    "    class_names = os.listdir(os.path.join(DATA_ROOT, dataset_name, \"train\"))\n",
    "    class_names = [int(name) for name in class_names]\n",
    "    class_names.sort()\n",
    "    class_names = [str(name) for name in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_classes_dict(dataset_name):\n",
    "    class_names = get_classes(dataset_name)\n",
    "    class_names_dict = {}\n",
    "    for class_name in class_names:\n",
    "        image_sample_name = os.path.basename(glob.glob(os.path.join(DATA_ROOT, dataset_name, \"train\", str(class_name),'*'))[0])\n",
    "        class_names_dict[str(class_name)] = image_sample_name.split('_')[-2]\n",
    "    return class_names_dict\n",
    "\n",
    "def get_classes_dict_empty_lists(dataset_name):\n",
    "    class_names = get_classes(dataset_name)\n",
    "    class_names_dict = {}\n",
    "    for class_name in class_names:\n",
    "        class_names_dict[str(class_name)] = []\n",
    "    return class_names_dict\n",
    "\n",
    "def get_label(file_path):\n",
    "    # Convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    # TODO: pass CLASS_NAMES as function parameter\n",
    "    one_hot = parts[-2] == CLASS_NAMES\n",
    "    # Integer encode the label\n",
    "    return int(one_hot)\n",
    "\n",
    "def decode_img(img):\n",
    "    # Convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    return img\n",
    "\n",
    "def aug_fn(image):\n",
    "    data = {\"image\":image}\n",
    "    aug_data = transforms(**data)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    return aug_img\n",
    "\n",
    "def apply_transformations(image):\n",
    "    aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n",
    "    return aug_img\n",
    "\n",
    "def process_path(file_path, use_augmentations: bool):\n",
    "    label = get_label(file_path)\n",
    "    # Load the raw data from the file as a string\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = decode_img(image)\n",
    "    # Resize and norm the image\n",
    "    image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n",
    "    image = tf.cast(image/255, tf.float32)\n",
    "    if use_augmentations:\n",
    "        image = apply_transformations(image)\n",
    "    return image, label\n",
    "\n",
    "def load_dataset(dataset_name:str, split_type: str, use_transformations: bool, mini_batch_size: int, shuffle: bool):\n",
    "    global CLASS_NAMES\n",
    "    CLASS_NAMES = get_classes(dataset_name)\n",
    "    dataset = tf.data.Dataset.list_files(f\"{DATA_ROOT}/{dataset_name}/{split_type}/*/*\", shuffle=False)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(dataset), reshuffle_each_iteration=False)\n",
    "    dataset = dataset.map(partial(process_path, use_augmentations=use_transformations), num_parallel_calls=AUTOTUNE).batch(mini_batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b097b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.applications import EfficientNetB5\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_initial_weights_type(imagenet_pretrained_backbone: bool):\n",
    "    if imagenet_pretrained_backbone:\n",
    "        weights='imagenet'\n",
    "    else:\n",
    "        weights=None\n",
    "    return weights\n",
    "\n",
    "def rebuild_top(model, class_count: int, top_dropout_rate: float = 0):\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.BatchNormalization(name='batch_normalization')(x)\n",
    "    if top_dropout_rate != 0:\n",
    "        x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(class_count, activation=\"softmax\", name=\"pred\")(x)\n",
    "    return outputs\n",
    "\n",
    "def build_EffNetB0(class_count: int, imagenet_pretrained_backbone: bool, top_dropout_rate: float = 0):\n",
    "    \n",
    "    weights_type = get_initial_weights_type(imagenet_pretrained_backbone)\n",
    "    \n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=weights_type)\n",
    "    outputs = rebuild_top(model, class_count, top_dropout_rate)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='EfficientNetB0')\n",
    "    print(\"EffNet0 model build successfull\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_EffNetB3(class_count: int, imagenet_pretrained_backbone: bool, top_dropout_rate: float = 0):\n",
    "    \n",
    "    weights_type = get_initial_weights_type(imagenet_pretrained_backbone)\n",
    "    \n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    model = EfficientNetB3(include_top=False, input_tensor=inputs, weights=weights_type)\n",
    "    outputs = rebuild_top(model, class_count, top_dropout_rate)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='EfficientNetB3')\n",
    "    print(\"EffNet3 model build successfull\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_EffNetB5(class_count: int, imagenet_pretrained_backbone: bool, top_dropout_rate: float = 0):\n",
    "    \n",
    "    weights_type = get_initial_weights_type(imagenet_pretrained_backbone)\n",
    "    \n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    model = EfficientNetB5(include_top=False, input_tensor=inputs, weights=weights_type)\n",
    "    outputs = rebuild_top(model, class_count, top_dropout_rate)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='EfficientNetB5')\n",
    "    print(\"EffNet5 model build successfull\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_model(class_count: int, imagenet_pretrained_backbone: bool, architecture: str,  top_dropout_rate: float = 0):\n",
    "    if architecture == \"EffNetB0\":\n",
    "        model = build_EffNetB0(class_count, imagenet_pretrained_backbone, top_dropout_rate)\n",
    "    elif architecture == \"EffNetB3\":\n",
    "        model = build_EffNetB3(class_count, imagenet_pretrained_backbone, top_dropout_rate)\n",
    "    elif architecture == \"EffNetB5\":\n",
    "        model = build_EffNetB5(class_count, imagenet_pretrained_backbone, top_dropout_rate)\n",
    "    else:\n",
    "        raise Exception(f\"Specified model architecture is not available\")\n",
    "        \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712defe8",
   "metadata": {},
   "source": [
    "## Visiaulize samples from downstream taskdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"data_v10.0.0\"\n",
    "\n",
    "map_dict = get_classes_dict(dataset_name)\n",
    "sample_dict = get_classes_dict_empty_lists(dataset_name)\n",
    "\n",
    "rot_train_dataset_eg = load_dataset(\n",
    "                              dataset_name = dataset_name, \n",
    "                              split_type = \"train\", \n",
    "                              use_transformations = True, \n",
    "                              mini_batch_size = 1,\n",
    "                              shuffle=True)\n",
    "\n",
    "iterator=iter(rot_train_dataset_eg)\n",
    "\n",
    "for _ in range(200):\n",
    "    sample = next(iterator)\n",
    "    image = sample[0].numpy().reshape(IMG_SIZE,IMG_SIZE,3)\n",
    "    label = tf.math.argmax(sample[1], axis=1).numpy()[0]\n",
    "    sample_dict[str(label)].append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = len(sample_dict)\n",
    "plt.figure(figsize=(14,2*class_count))\n",
    "i=1\n",
    "for row in range(class_count):\n",
    "    for column in range(7):\n",
    "        plt.subplot(class_count, 7, i)\n",
    "        image = sample_dict[str(row)][column]\n",
    "        classs = map_dict[str(row)]\n",
    "        plt.imshow(image)\n",
    "        plt.title(f'{classs}')\n",
    "        plt.axis('off')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7de835",
   "metadata": {},
   "source": [
    "## Fine-tune pretrained model for brain tumor classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "770be28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "major_version = 10\n",
    "minor_version = 3\n",
    "config_dict={\"type\": \"SSL\",\n",
    "             \"task\": \"Rotation angle classification\",\n",
    "             \"dataset_origin\": \"Pneumothorax\",\n",
    "             \"dataset_name\": f\"data_v10.0.0\",\n",
    "             \"architecture\": \"EffNetB3\",\n",
    "             \"continue_training_from\": \"\",\n",
    "             \"imagenet_pretrained_backbone\": False,\n",
    "             \"use_transformations\": True,\n",
    "             \"loss_function\": \"categorical_crossentropy\",\n",
    "             \"top_dropout_rate\": 0.2,\n",
    "             \"metrics\": ['acc'],\n",
    "             \"learning_rate\": 0.0005,\n",
    "             \"batch_size\": 32,\n",
    "             \"epochs\": 100}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"model_v{str(major_version)}.{str(minor_version)}.0\"\n",
    "\n",
    "run = wandb.init(entity = 'ssl_bakalauras',\n",
    "                 project=f\"SSL_v{str(major_version)}\",\n",
    "                 name = model_name,\n",
    "                 config = config_dict)\n",
    "\n",
    "config = wandb.config\n",
    "project_name = run.project\n",
    "\n",
    "print(config)\n",
    "print(f\"\\nModel version: {model_name}\\n\")\n",
    "print(f\"{config.architecture} architecture will be used\")\n",
    "\n",
    "path_to_model = os.path.join(MODEL_ROOT, project_name, model_name)\n",
    "os.makedirs(path_to_model)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class_names = get_classes_dict(config.dataset_name)\n",
    "print(f\"\\nClasses: {class_names}\\n\")\n",
    "if config.use_transformations:\n",
    "    transformations = str(transforms)\n",
    "    print(f\"Using transfromations for train dataset:{transformations}\")\n",
    "else:\n",
    "    transformations = None\n",
    "    print(f\"No transformations will be used\")\n",
    "\n",
    "\n",
    "wandb.log({\n",
    "    \"class_names\": str(class_names),\n",
    "    \"transformations\": transformations\n",
    "})\n",
    "\n",
    "train_ds = load_dataset(\n",
    "                      dataset_name = config.dataset_name, \n",
    "                      split_type = \"train\", \n",
    "                      use_transformations = config.use_transformations, \n",
    "                      mini_batch_size = config.batch_size,\n",
    "                      shuffle=True)\n",
    "\n",
    "val_ds = load_dataset(\n",
    "                      dataset_name = config.dataset_name, \n",
    "                      split_type = \"val\", \n",
    "                      use_transformations = config.use_transformations, \n",
    "                      mini_batch_size = config.batch_size,\n",
    "                      shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "if config.continue_training_from:\n",
    "    print(f\"Continue training from: {config.continue_training_from}\")\n",
    "    model = tf.keras.models.load_model(os.path.join(MODEL_ROOT, f\"SSL_v{str(major_version)}\", config.continue_training_from))\n",
    "else:                   \n",
    "    model = build_model(\n",
    "                    class_count = len(class_names), \n",
    "                    imagenet_pretrained_backbone = config.imagenet_pretrained_backbone,\n",
    "                    architecture = config.architecture,\n",
    "                    top_dropout_rate = config.top_dropout_rate)\n",
    "    print(f\"Successfully initialized {config.architecture} model from scratch\")\n",
    "\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=config.loss_function, metrics = config.metrics)\n",
    "\n",
    "model.summary()\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(os.path.join(path_to_model, \"checkpoints\", \"weights{epoch:08d}.ckpt\"), \n",
    "                                     save_weights_only=True, save_freq=191, verbose=1)\n",
    "model.fit(train_ds,\n",
    "          epochs=config.epochs,\n",
    "          validation_data=val_ds,\n",
    "          callbacks=[WandbCallback(save_model=(False)), mc])\n",
    "\n",
    "model.save(path_to_model)\n",
    "\n",
    "run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
