{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c401d1",
   "metadata": {},
   "source": [
    "# Purpose of this notebook is to generate model activation heatmaps video for all duration of model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d02246",
   "metadata": {},
   "source": [
    "## Setup the constants and run whole notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b817f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/home/gpu1/Desktop/kamilio/Pneumothorax\"\n",
    "dataset_version = \"data_v4.0.0\"\n",
    "split_type = \"train\"\n",
    "IMG_SIZE = 256\n",
    "\n",
    "MODEL_ROOT = \"/home/gpu1/Desktop/kamilio/models\"\n",
    "model_category = \"SSL_v10\"\n",
    "model_version = \"model_v10.3.0\"\n",
    "\n",
    "# Set \"activation_heatmaps\" or \"feature_embeddings\" (\"feature embeddings\"  to be added)\n",
    "type_of_video = \"feature_embeddings\"\n",
    "\n",
    "# if type_of_video==\"activation_heatmaps\" specify\n",
    "images_per_class = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab639e",
   "metadata": {},
   "source": [
    "## Start of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d8d57",
   "metadata": {},
   "source": [
    "### Loading necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1945e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import print_function  # for Python2\n",
    "import sys\n",
    "\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "replace2linear = ReplaceToLinear()\n",
    "from tf_keras_vis.utils.scores import CategoricalScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72442632",
   "metadata": {},
   "source": [
    "### Creating path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f61af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = os.path.join(MODEL_ROOT, model_category, model_version)\n",
    "path_to_model_checkpoints = os.path.join(MODEL_ROOT, model_category, model_version, \"checkpoints\")\n",
    "if type_of_video == \"activation_heatmaps\":\n",
    "    path_to_video = os.path.join(MODEL_ROOT, model_category, model_version, \"activation_heatmaps\")\n",
    "    path_to_video_frames = os.path.join(MODEL_ROOT, model_category, model_version, \"activation_heatmaps\", \"frames\")\n",
    "elif type_of_video == \"feature_embeddings\":\n",
    "    path_to_video = os.path.join(MODEL_ROOT, model_category, model_version, \"feature_embeddings\")\n",
    "    path_to_video_frames = os.path.join(MODEL_ROOT, model_category, model_version, \"feature_embeddings\", \"frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c263a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = os.listdir(path_to_model_checkpoints)\n",
    "# files1 = [file for file in files if file.endswith(\".index\")]\n",
    "# files2 = [file for file in files if file.endswith(\"001\")]\n",
    "# for num, file in enumerate(files2):\n",
    "#     file_new = file[:12] + str(int(num)+501) + file[15:]\n",
    "#     src = os.path.join(path_to_model_checkpoints, file)\n",
    "#     dst = os.path.join(path_to_model_checkpoints, file_new)\n",
    "#     os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7995038e",
   "metadata": {},
   "source": [
    "### Creating directories for frames and video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "119df450",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path_to_video)\n",
    "os.makedirs(path_to_video_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9940b",
   "metadata": {},
   "source": [
    "### Loading necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "516aecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "from functools import partial\n",
    "\n",
    "transforms = A.Compose([\n",
    "])\n",
    "\n",
    "def get_classes(dataset_name):\n",
    "    #TODO add functionality to pop error if classes in data splits do not match\n",
    "    class_names = os.listdir(os.path.join(DATA_ROOT, dataset_name, \"train\"))\n",
    "    class_names = [int(name) for name in class_names]\n",
    "    class_names.sort()\n",
    "    class_names = [str(name) for name in class_names]\n",
    "    print(class_names)\n",
    "    return class_names\n",
    "\n",
    "\n",
    "def get_classes_dict(dataset_name):\n",
    "    class_names = get_classes(dataset_name)\n",
    "    class_names_dict = {}\n",
    "    for class_name in class_names:\n",
    "        image_sample_name = os.path.basename(glob.glob(os.path.join(DATA_ROOT, dataset_name, \"train\", str(class_name),'*'))[0])\n",
    "        class_names_dict[str(class_name)] = image_sample_name.split('_')[-2]\n",
    "    return class_names_dict\n",
    "\n",
    "def get_classes_dict_empty_lists(dataset_name):\n",
    "    class_names = get_classes(dataset_name)\n",
    "    class_names_dict = {}\n",
    "    for class_name in class_names:\n",
    "        class_names_dict[str(class_name)] = []\n",
    "    return class_names_dict\n",
    "\n",
    "def get_label(file_path):\n",
    "    # Convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    # TODO: pass CLASS_NAMES as function parameter\n",
    "    one_hot = parts[-2] == CLASS_NAMES\n",
    "    # Integer encode the label\n",
    "    return int(one_hot)\n",
    "\n",
    "def decode_img(img):\n",
    "    # Convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    return img\n",
    "\n",
    "def aug_fn(image):\n",
    "    data = {\"image\":image}\n",
    "    aug_data = transforms(**data)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    # Resize and norm the image\n",
    "    aug_img = tf.image.resize(aug_img, size=[IMG_SIZE, IMG_SIZE])\n",
    "    return aug_img\n",
    "\n",
    "def apply_transformations(image):\n",
    "    aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n",
    "    return aug_img\n",
    "\n",
    "def process_path(file_path, use_augmentations: bool):\n",
    "    label = get_label(file_path)\n",
    "    # Load the raw data from the file as a string\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = decode_img(image)\n",
    "    image = tf.cast(image/255, tf.float32)\n",
    "    if use_augmentations:\n",
    "        image = apply_transformations(image)\n",
    "    return image, label\n",
    "\n",
    "def load_dataset(dataset_name:str, split_type: str, use_transformations: bool, mini_batch_size: int):\n",
    "    global CLASS_NAMES\n",
    "    CLASS_NAMES = get_classes(dataset_name)\n",
    "    dataset = tf.data.Dataset.list_files(f\"{DATA_ROOT}/{dataset_name}/{split_type}/*/*\", shuffle=False)\n",
    "    dataset = dataset.shuffle(len(dataset), reshuffle_each_iteration=False)\n",
    "    dataset = dataset.map(partial(process_path, use_augmentations=use_transformations), num_parallel_calls=AUTOTUNE).batch(mini_batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d3f2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.applications import EfficientNetB5\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_initial_weights_type(imagenet_pretrained_backbone: bool):\n",
    "    if imagenet_pretrained_backbone:\n",
    "        weights='imagenet'\n",
    "    else:\n",
    "        weights=None\n",
    "    return weights\n",
    "\n",
    "def rebuild_top(model, class_count: int, top_dropout_rate: float = 0):\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.BatchNormalization(name='batch_normalization')(x)\n",
    "    if top_dropout_rate != 0:\n",
    "        x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(class_count, activation=\"softmax\", name=\"pred\")(x)\n",
    "    return outputs\n",
    "\n",
    "def build_EffNetB0(class_count: int, imagenet_pretrained_backbone: bool, top_dropout_rate: float = 0):\n",
    "    \n",
    "    weights_type = get_initial_weights_type(imagenet_pretrained_backbone)\n",
    "    \n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=weights_type)\n",
    "    outputs = rebuild_top(model, class_count, top_dropout_rate)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='EfficientNetB0')\n",
    "    print(\"EffNet0 model build successfull\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_EffNetB3(class_count: int, imagenet_pretrained_backbone: bool, top_dropout_rate: float = 0):\n",
    "    \n",
    "    weights_type = get_initial_weights_type(imagenet_pretrained_backbone)\n",
    "    \n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    model = EfficientNetB3(include_top=False, input_tensor=inputs, weights=weights_type)\n",
    "    outputs = rebuild_top(model, class_count, top_dropout_rate)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='EfficientNetB3')\n",
    "    print(\"EffNet3 model build successfull\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_EffNetB5(class_count: int, imagenet_pretrained_backbone: bool, top_dropout_rate: float = 0):\n",
    "    \n",
    "    weights_type = get_initial_weights_type(imagenet_pretrained_backbone)\n",
    "    \n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    model = EfficientNetB5(include_top=False, input_tensor=inputs, weights=weights_type)\n",
    "    outputs = rebuild_top(model, class_count, top_dropout_rate)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='EfficientNetB5')\n",
    "    print(\"EffNet5 model build successfull\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_model(class_count: int, imagenet_pretrained_backbone: bool, architecture: str,  top_dropout_rate: float = 0):\n",
    "    if architecture == \"EffNetB0\":\n",
    "        model = build_EffNetB0(class_count, imagenet_pretrained_backbone, top_dropout_rate)\n",
    "    elif architecture == \"EffNetB3\":\n",
    "        model = build_EffNetB3(class_count, imagenet_pretrained_backbone, top_dropout_rate)\n",
    "    elif architecture == \"EffNetB5\":\n",
    "        model = build_EffNetB5(class_count, imagenet_pretrained_backbone, top_dropout_rate)\n",
    "    else:\n",
    "        raise Exception(f\"Specified model architecture is not available\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876eaa59",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb25f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = get_classes_dict(dataset_version)\n",
    "sample_dict = get_classes_dict_empty_lists(dataset_version)\n",
    "class_count = len(get_classes(dataset_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788be10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if type_of_video == \"feature_embeddings\":\n",
    "    ds = load_dataset(\n",
    "                      dataset_name = dataset_version, \n",
    "                      split_type = split_type, \n",
    "                      use_transformations = False, \n",
    "                      mini_batch_size = 64)\n",
    "    \n",
    "    y_true = np.concatenate([y for x, y in ds], axis=0)\n",
    "    y_true = [str(y[1]) for y in y_true]\n",
    "    y_true_int = np.array([int(y) for y in y_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2b86cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type_of_video == \"activation_heatmaps\":\n",
    "    ds = load_dataset(\n",
    "                      dataset_name = dataset_version, \n",
    "                      split_type = split_type, \n",
    "                      use_transformations = False, \n",
    "                      mini_batch_size = 1)\n",
    "    iterator = iter(ds)\n",
    "    \n",
    "    for _ in range(len(ds)):\n",
    "        sample = next(iterator)\n",
    "        image = sample[0].numpy().reshape(IMG_SIZE,IMG_SIZE,3)\n",
    "        label = tf.math.argmax(sample[1], axis=1).numpy()[0]\n",
    "        sample_dict[str(label)].append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69b390c",
   "metadata": {},
   "source": [
    "### Generating frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d4bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.models.load_model(path_to_model)\n",
    "print(model_version)\n",
    "print('\\n')\n",
    "checkpoints = os.listdir(path_to_model_checkpoints)\n",
    "checkpoints = [f\"{x.split('.')[0]}.ckpt\" for x in checkpoints if x.endswith(\".index\")]\n",
    "epoch_nmi = []\n",
    "\n",
    "# checkpoints = [\"weights00000001.ckpt\",\"weights00000138.ckpt\",\"weights00000500.ckpt\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if type_of_video == \"activation_heatmaps\":\n",
    "    print(\"Generating model activation heatmap frames\")\n",
    "elif type_of_video == \"feature_embeddings\":\n",
    "    print(\"Generating model feature embeddings frames\")\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    epoch = int(checkpoint.split('.')[0][-4:])\n",
    "    start = time.time()\n",
    "    model.load_weights(os.path.join(path_to_model_checkpoints, checkpoint))\n",
    "    print(f\"checkpoint: {checkpoint}\")\n",
    "\n",
    "    if type_of_video == \"activation_heatmaps\":\n",
    "\n",
    "        plt.figure(figsize=(3*class_count, 3.5*images_per_class))\n",
    "#         plt.suptitle(f\"Activation heatmaps of {model_version}. Checkpoint: {checkpoint.split('.')[0]}\", fontsize=24)\n",
    "        plt.suptitle(f\"Checkpoint: {checkpoint.split('.')[0]}\", fontsize=24)\n",
    "        i=1\n",
    "\n",
    "        # Create GradCAM++ object\n",
    "        gradcam = Gradcam(model,\n",
    "                                  model_modifier=replace2linear,\n",
    "                                  clone=True)\n",
    "\n",
    "        for row in range(images_per_class):\n",
    "            for column in range(class_count):\n",
    "                # Generate heatmap with GradCAM++\n",
    "                cam = gradcam(CategoricalScore([int(column)]),\n",
    "                              [sample_dict[str(column)][row]],\n",
    "                              penultimate_layer=-1)\n",
    "                heatmap = np.uint8(cm.jet(cam[0])[..., :3] * 255).reshape(IMG_SIZE,IMG_SIZE,3)\n",
    "\n",
    "                plt.subplot(images_per_class, class_count, i)\n",
    "                image = sample_dict[str(column)][row]\n",
    "                classs = map_dict[str(column)].replace('-', ',')\n",
    "                plt.imshow(image)\n",
    "                plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "                if row == 0:\n",
    "                    plt.title(f'Degrees: {classs}', fontsize=16)\n",
    "                if not(row==0 and column==0):\n",
    "                    plt.axis('off')\n",
    "                i += 1\n",
    "        plot_name =  f\"{checkpoint.split('.')[0]}.jpg\"\n",
    "        plt.savefig(os.path.join(path_to_video_frames, plot_name))\n",
    "        plt.close()\n",
    "        print(f\"Saved image:  {plot_name}\\n\")\n",
    "        end = time.time()\n",
    "        print(f\"Duration: {round(end-start, 2)} seconds\\n\")\n",
    "\n",
    "    elif type_of_video == \"feature_embeddings\":\n",
    "        # Reinitializing dense classification layer\n",
    "        inputs = model.input\n",
    "        x = model.layers[-5].output\n",
    "        x_GAP = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "\n",
    "        model_GAP = tf.keras.Model(inputs, x_GAP)\n",
    "\n",
    "        features = np.array(model_GAP.predict(ds, verbose = 0))\n",
    "        features = features.T\n",
    "        print('Fitting PCA')\n",
    "        pca = PCA()\n",
    "        pca.fit(features)\n",
    "\n",
    "        first_component = pca.components_[0,:]\n",
    "        second_component = pca.components_[1,:]\n",
    "        PCA_first_two_components = np.concatenate((first_component.reshape(-1, 1), second_component.reshape(-1, 1)), axis=1)\n",
    "        SpectralClustering_model = SpectralClustering(n_clusters=2)\n",
    "        SpectralClustering_pred = SpectralClustering_model.fit_predict(pca.components_[0:10,:].T)\n",
    "        nmi = normalized_mutual_info_score(y_true_int, SpectralClustering_pred)\n",
    "        nmi_rounded = round(nmi, 5)\n",
    "\n",
    "        print(f\"NMI: {str(nmi_rounded)}\")\n",
    "\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({\"first\": first_component, \"second\": second_component, \"y_true\": y_true, \"SpectralClustering_pred\": [str(y) for y in SpectralClustering_pred]})\n",
    "        class_1_count = len(df[\"SpectralClustering_pred\"][df[\"SpectralClustering_pred\"]=='1'])\n",
    "        epoch_nmi.append([epoch, nmi_rounded])\n",
    "        colors = {\"0\": \"red\", \"1\": \"green\"}\n",
    "\n",
    "        # Create two subplots side by side\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        # Plot y_true in the first subplot\n",
    "        ax1.scatter(df['first'], df['second'], c=df['y_true'].map(colors), s=1.5)\n",
    "        ax1.set_title(f\"True labels\")\n",
    "        ax1.set_xlabel('first component')\n",
    "        ax1.set_ylabel('second component')\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot DBSCAN_pred in the second subplot\n",
    "        ax2.scatter(df['first'], df['second'],c=df['SpectralClustering_pred'].map(colors), s=1.5)\n",
    "        ax2.set_title(f\"SpectralClustering. First 10 PCA components. (NMI: {str(nmi_rounded)})\")\n",
    "        ax2.set_xlabel('first component')\n",
    "        ax2.set_ylabel('second component')\n",
    "        ax2.legend()\n",
    "\n",
    "        # Add a shared title to the subplots\n",
    "        fig.suptitle(f\"{model_version} feature embeddings. First two components of PCA. Checkpoint: {checkpoint.split('.')[0]}\")\n",
    "\n",
    "\n",
    "        # Save the plot and close the figure\n",
    "        plot_name = f\"{checkpoint.split('.')[0]}.jpg\"\n",
    "        plt.savefig(os.path.join(path_to_video_frames, plot_name))\n",
    "        plt.close()\n",
    "        print(f\"Saved image: {plot_name}\")\n",
    "\n",
    "#         colors = {\"0\":'red', \"1\":'green'}\n",
    "#         plt.scatter(df['first'], df['second'], c=df['label'].map(colors), s=1.5)\n",
    "#         plt.title(f\"{model_version} feature embeddings. First two components of PCA.\\n Checkpoint: {checkpoint.split('.')[0]}. NMI:{str(nmi_rounded)}\")\n",
    "#         plt.xlabel('first component')\n",
    "#         plt.ylabel('second component')\n",
    "#         plot_name =  f\"{checkpoint.split('.')[0]}.jpg\"\n",
    "#         plt.savefig(os.path.join(path_to_video_frames, plot_name))\n",
    "#         plt.close()\n",
    "#         print(f\"Saved image:  {plot_name}\")\n",
    "        end = time.time()\n",
    "        print(f\"Duration: {round(end-start, 2)} seconds\\n\")\n",
    "\n",
    "if type_of_video == \"feature_embeddings\":\n",
    "    nmi_title =  f\"NMI_{model_version}_{dataset_version}_{split_type}\"\n",
    "    epoch_nmi = np.array(epoch_nmi)\n",
    "    epoch_nmi_df = pd.DataFrame({\"Epoch\": epoch_nmi[:,0],\"NMI\": epoch_nmi[:,1]})\n",
    "    max_row_index = epoch_nmi_df[\"NMI\"].idxmax()\n",
    "    max_row = epoch_nmi_df.iloc[max_row_index]\n",
    "    epoch_nmi_df.plot(x=\"Epoch\", y=\"NMI\", kind=\"line\")\n",
    "    epoch_nmi_df.to_csv(os.path.join(path_to_model, f\"{nmi_title}_SpectralClustering_test.csv\"))\n",
    "\n",
    "    # Add axis labels and a title\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"NMI\")\n",
    "    plt.grid()\n",
    "    plt.title(f\"NMI vs Epoch. Best epoch: {str(int(max_row[0]))} (NMI={str(max_row[1])})\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(os.path.join(path_to_model, f\"{nmi_title}.jpg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d74330",
   "metadata": {},
   "source": [
    "### Generating video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d619f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all the filenames of the frames in the directory\n",
    "frame_filenames = os.listdir(path_to_video_frames)\n",
    "\n",
    "# Sort the filenames in ascending order\n",
    "frame_filenames.sort()\n",
    "\n",
    "# Get the first frame to get its size\n",
    "frame = cv2.imread(os.path.join(path_to_video_frames, frame_filenames[0]))\n",
    "height, width, channels = frame.shape\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(os.path.join(path_to_video, \"video.mp4\"), fourcc, 5.0, (width, height))\n",
    "\n",
    "# Loop through all the frames and write them to the output video\n",
    "for filename in frame_filenames:\n",
    "    frame_path = os.path.join(path_to_video_frames, filename)\n",
    "    frame = cv2.imread(frame_path)\n",
    "    out.write(frame)\n",
    "\n",
    "# Release everything if job is finished\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62914ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "start = time.time()\n",
    "model = build_model(\n",
    "                    class_count = 2, \n",
    "                    imagenet_pretrained_backbone = True,\n",
    "                    architecture = 'EffNetB0',\n",
    "                    top_dropout_rate = 0.2)\n",
    "inputs = model.input\n",
    "x = model.layers[-5].output\n",
    "x_GAP = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "\n",
    "model_GAP = tf.keras.Model(inputs, x_GAP)\n",
    "\n",
    "features = np.array(model_GAP.predict(ds, verbose = 0))\n",
    "features = features.T\n",
    "print('Fitting PCA')\n",
    "pca = PCA()\n",
    "pca.fit(features)\n",
    "\n",
    "first_component = pca.components_[0,:]\n",
    "second_component = pca.components_[1,:]\n",
    "PCA_first_two_components = np.concatenate((first_component.reshape(-1, 1), second_component.reshape(-1, 1)), axis=1)\n",
    "SpectralClustering_model = SpectralClustering(n_clusters=2)\n",
    "SpectralClustering_pred = SpectralClustering_model.fit_predict(pca.components_[0:10,:].T)\n",
    "nmi = normalized_mutual_info_score(y_true_int, SpectralClustering_pred)\n",
    "nmi_rounded = round(nmi, 5)\n",
    "\n",
    "print(f\"NMI: {str(nmi_rounded)}\")\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"first\": first_component, \"second\": second_component, \"y_true\": y_true, \"SpectralClustering_pred\": [str(y) for y in SpectralClustering_pred]})\n",
    "class_1_count = len(df[\"SpectralClustering_pred\"][df[\"SpectralClustering_pred\"]=='1'])\n",
    "colors = {\"0\": \"red\", \"1\": \"green\"}\n",
    "\n",
    "# Create two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot y_true in the first subplot\n",
    "ax1.scatter(df['first'], df['second'], c=df['y_true'].map(colors), s=1.5, label=['pos', 'neg'])\n",
    "ax1.set_title(f\"True labels\")\n",
    "ax1.set_xlabel('first component')\n",
    "ax1.set_ylabel('second component')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot DBSCAN_pred in the second subplot\n",
    "ax2.scatter(df['first'], df['second'],c=df['SpectralClustering_pred'].map(colors), s=1.5)\n",
    "ax2.set_title(f\"SpectralClustering. First 10 PCA components. (NMI: {str(nmi_rounded)})\")\n",
    "ax2.set_xlabel('first component')\n",
    "ax2.set_ylabel('second component')\n",
    "ax2.legend()\n",
    "\n",
    "# Add a shared title to the subplots\n",
    "fig.suptitle(f\"ImageNet model feature embeddings. First two components of PCA.\")\n",
    "\n",
    "\n",
    "# Save the plot and close the figure\n",
    "plot_name =  f\"ImageNet.jpg\"\n",
    "plt.savefig(os.path.join(path_to_video_frames, plot_name))\n",
    "plt.close()\n",
    "print(f\"Saved image:  {plot_name}\")\n",
    "end = time.time()\n",
    "print(f\"Duration: {round(end-start, 2)} seconds\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
